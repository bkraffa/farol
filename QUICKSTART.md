# üöÄ Guia de In√≠cio R√°pido

## Instala√ß√£o em 5 minutos

### 1. Clone o reposit√≥rio
```bash
git clone <seu-repo>
cd kitesurf-scraper
```

### 2. Instale as depend√™ncias
```bash
pip install -r requirements.txt
```

### 3. Inicie o MongoDB (Local)

**Op√ß√£o A: Com Docker (Recomendado)**
```bash
docker-compose up -d
```

**Op√ß√£o B: MongoDB local**
```bash
# macOS
brew services start mongodb-community

# Linux
sudo systemctl start mongod
```

### 4. Configure suas chaves de API

Crie o arquivo `config/.env`:
```bash
cp config/.env.example config/.env
```

Edite `config/.env` e adicione:
```env
APIFY_API_TOKEN=seu_token_aqui
OPENAI_API_KEY=sua_chave_aqui
OPENAI_MODEL=gpt-4o-mini
```

#### Como obter as chaves:

**Apify:**
1. Acesse https://apify.com
2. Crie uma conta gratuita
3. V√° em Settings > Integrations > API tokens
4. Copie seu token

**OpenAI:**
1. Acesse https://platform.openai.com
2. Crie uma conta
3. V√° em API Keys
4. Crie uma nova chave
5. Adicione cr√©ditos (m√≠nimo $5)

### 4. Configure os grupos

Edite `config/groups.json` e adicione os grupos do Facebook:
```json
{
  "groups": [
    {
      "url": "https://www.facebook.com/groups/SEU_GRUPO",
      "name": "Nome do Grupo",
      "active": true
    }
  ]
}
```

### 5. Execute o scraping hist√≥rico

```bash
python scripts/run_historical.py
```

Isso vai:
- Buscar posts dos √∫ltimos 2 anos
- Analisar todos com OpenAI
- Salvar resultados em `data/analyzed/`

### 6. Configure execu√ß√£o autom√°tica

Para rodar 2x por dia automaticamente:

```bash
# Torne o script execut√°vel
chmod +x scripts/schedule_jobs.sh

# Edite o script com o caminho correto
nano scripts/schedule_jobs.sh

# Adicione ao crontab
crontab -e

# Adicione esta linha:
0 8,20 * * * /path/to/kitesurf-scraper/scripts/schedule_jobs.sh
```

## üìä Visualizando Resultados

### Via MongoDB UI

Acesse **Mongo Express** em http://localhost:8081
- User: `admin`
- Password: `admin`

### Via Script de Query

```bash
# Ver estat√≠sticas
python scripts/query_db.py stats

# Buscar an√∫ncios
python scripts/query_db.py search brand=Duotone max_price=5000

# An√∫ncios recentes
python scripts/query_db.py recent 24

# Exportar para CSV
python scripts/query_db.py export meus_anuncios.csv
```

### Campos no Banco

Os resultados s√£o salvos em `data/analyzed/historical_*.json` - Dados completos
- `data/analyzed/historical_*_summary.csv` - CSV resumido (Excel)
- `data/analyzed/historical_*_stats.json` - Estat√≠sticas

| Campo | Descri√ß√£o |
|-------|-----------|
| equipment_type | Tipo (kite, board, bar, etc) |
| brand | Marca (Duotone, North, etc) |
| model | Modelo espec√≠fico |
| year | Ano do equipamento |
| size | Tamanho (12m, 136x41, etc) |
| price | Pre√ßo em reais |
| condition | Estado (novo, usado, etc) |
| city | Cidade |
| state | Estado (CE, SP, RJ, etc) |
| has_repair | Tem reparo? (true/false) |
| confidence_score | Confian√ßa da an√°lise (0-1) |

## üí° Dicas

### Economizar custos OpenAI

Use `gpt-4o-mini` (mais barato) em `.env`:
```env
OPENAI_MODEL=gpt-4o-mini
```

Custo aproximado:
- gpt-4o-mini: ~$0.01 por 1000 posts
- gpt-4o: ~$0.15 por 1000 posts

### Testar com poucos posts

Para testar, limite o n√∫mero de posts no Apify:
```python
# Em apify_scraper.py, linha 20
"maxPosts": 10,  # Altere de 10000 para 10
```

### Monitorar execu√ß√£o

```bash
# Ver √∫ltimos logs
tail -f logs/incremental_*.log

# Ver estat√≠sticas do √∫ltimo run
cat data/analyzed/*_stats.json | jq
```

## üÜò Problemas Comuns

### "APIFY_API_TOKEN not found"
‚Üí Crie o arquivo `config/.env` com suas chaves

### "No module named 'apify_client'"
‚Üí Execute: `pip install -r requirements.txt`

### "Rate limit exceeded" (OpenAI)
‚Üí Adicione delay entre an√°lises ou reduza batch size

### "No posts found"
‚Üí Verifique se os grupos est√£o p√∫blicos e o Apify consegue acessar

## üìû Suporte

- Documenta√ß√£o completa: `README.md`
- Exemplos de c√≥digo: `examples/usage_examples.py`
- Issues: (adicione link do seu repo)
