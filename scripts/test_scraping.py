#!/usr/bin/env python3
"""
Script de teste para scraping r√°pido (poucos posts)
√ötil para testar configura√ß√£o sem gastar cr√©ditos
"""
import os
import sys
import json
import logging
from pathlib import Path
from dotenv import load_dotenv
from datetime import datetime

# Adicionar diret√≥rio raiz ao path
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

from src.apify_scraper import ApifyFacebookScraper, load_groups_config

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """Teste r√°pido de scraping"""
    print("=" * 80)
    print("TESTE R√ÅPIDO DE SCRAPING")
    print("=" * 80)
    
    # Carregar vari√°veis de ambiente
    env_path = root_dir / "config" / ".env"
    load_dotenv(env_path)
    
    apify_token = os.getenv("APIFY_API_TOKEN")
    if not apify_token:
        logger.error("APIFY_API_TOKEN n√£o encontrado no .env")
        return 1
    
    # Configura√ß√µes
    MAX_POSTS = int(input("\nüìä Quantos posts buscar? (recomendado: 5-20): ") or "10")
    HOURS_BACK = int(input("‚è∞ Quantas horas para tr√°s? (padr√£o: 24): ") or "24")
    
    # Carregar grupos
    config_path = root_dir / "config" / "groups.json"
    group_urls = load_groups_config(str(config_path))
    
    print(f"\n‚úì Grupos configurados: {len(group_urls)}")
    for i, url in enumerate(group_urls, 1):
        print(f"  {i}. {url}")
    
    # Confirmar
    confirm = input(f"\nüöÄ Buscar {MAX_POSTS} posts dos √∫ltimos {HOURS_BACK}h? (s/N): ")
    if confirm.lower() != 's':
        print("Cancelado.")
        return 0
    
    # Inicializar scraper
    scraper = ApifyFacebookScraper(apify_token)
    
    try:
        # Executar scraping
        logger.info(f"Iniciando scraping de {MAX_POSTS} posts...")
        
        run_input = {
            "startUrls": [{"url": url} for url in group_urls],
            "viewOption": "CHRONOLOGICAL",
            "onlyPostsNewerThan": f"{HOURS_BACK} hours",
            "maxPosts": MAX_POSTS,
            "resultsLimit": MAX_POSTS
        }
        
        result = scraper._run_scraper(run_input, "test")
        
        # Resultados
        items = result.get("items", [])
        print("\n" + "=" * 80)
        print(f"‚úì SCRAPING CONCLU√çDO: {len(items)} posts coletados")
        print("=" * 80)
        
        if not items:
            print("\n‚ö†Ô∏è  Nenhum post encontrado. Tente:")
            print("  - Aumentar o per√≠odo (hours_back)")
            print("  - Verificar se os grupos t√™m posts recentes")
            print("  - Verificar se as URLs est√£o corretas")
            return 0
        
        # Mostrar primeiros posts
        print(f"\nüìù PRIMEIROS {min(3, len(items))} POSTS:\n")
        
        for i, item in enumerate(items[:3], 1):
            # Pegar dados do post
            if "sharedPost" in item and item["sharedPost"]:
                shared = item["sharedPost"]
                text = shared.get("text", "")
                title = shared.get("title", "")
                price = shared.get("price", "")
                location = shared.get("location", "")
            else:
                text = item.get("text", "")
                title = item.get("title", "")
                price = item.get("price", "")
                location = item.get("location", "")
            
            print(f"POST {i}:")
            print(f"  T√≠tulo: {title or '(sem t√≠tulo)'}")
            print(f"  Texto: {text[:100]}..." if len(text) > 100 else f"  Texto: {text}")
            print(f"  Pre√ßo: {price or '(n√£o informado)'}")
            print(f"  Local: {location or '(n√£o informado)'}")
            print(f"  Likes: {item.get('likesCount', 0)}")
            print(f"  Coment√°rios: {item.get('commentsCount', 0)}")
            print(f"  URL: {item.get('url', 'N/A')}")
            print()
        
        # Salvar resultado
        output_dir = root_dir / "data" / "tests"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"test_scraping_{MAX_POSTS}posts.json"
        # Handler para serializar datetime
        def json_serial(obj):
            """JSON serializer para objetos n√£o serializ√°veis"""
            if isinstance(obj, datetime):
                return obj.isoformat()
            raise TypeError(f"Type {type(obj)} not serializable")

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=json_serial)
        
        print(f"‚úì Dados salvos em: {output_file}")
        print(f"\nüí° Para testar an√°lise desses dados, execute:")
        print(f"   python scripts/test_analysis.py {output_file}")
        
        return 0
        
    except Exception as e:
        logger.error(f"Erro no scraping: {str(e)}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
